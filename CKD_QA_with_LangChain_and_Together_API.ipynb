{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNNIA8RZP6e3lBvMxJYDDE9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wahidur028/QA-with-LLMs/blob/main/CKD_QA_with_LangChain_and_Together_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**CKD (Custom Knowledge Domain) QA with LangChain and Together API**"
      ],
      "metadata": {
        "id": "8xF8hZJZm89J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Credits: Sam Witteveen**\n",
        "####Twitter: https://twitter.com/Sam_Witteveen"
      ],
      "metadata": {
        "id": "6DXZsneYmQKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**The Pipeline for converting raw unstructured data into a QA chain using LangChain**\n",
        "\n",
        "- **Step 1. Loading:** First, data needs to be loaded. Structured data and Unstructured data can be loaded from many sources. As of today (August 18, 2023), 154 data loaders are available on the LangChain platform. **[link](https://integrations.langchain.com/)**.\n",
        "\n",
        "- **Step 2. Splitting:** Text splitters break documents into splits of specified size. Chunk size and chunk overlaping can be defined here.\n",
        "\n",
        "- **Step 3. Storage:** To look up the document splits, it needs to be stored where we can later look them up. The most common way to do this is to embed the contents of each document and then store the embedding and document in a vector store, with the embedding being used to index the document.  As of today (August 18, 2023), 40 vectorstores and 30 text embedding are available on the LangChain platform. **[link](https://integrations.langchain.com/)**.\n",
        "\n",
        "- **Step 4. Retrieval:** Retrieve relevant splits for any question using similarity search. Vectorstores are commonly used for retrieval, but they are not the only option. For example, SVMs can also be used. LangChain has many retrievers including, but not limited to, vectorstores. Some common ways to improve on vector similarity search include:\n",
        "  - *MultiQueryRetriever* generates variants of the input question to improve retrieval. **[link](https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever)**.\n",
        "  - *Max marginal relevance* selects for relevance and diversity among the retrieved documents. **[link](https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf)**.\n",
        "\n",
        "- **Step 5. Generation:** An LLM produces an answer using a prompt that includes the question and the retrieved data. You can pass in an LLM or a ChatModel to the RetrievalQA chain. **[link](https://integrations.langchain.com/)**."
      ],
      "metadata": {
        "id": "wwmZvKyU89Ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Package installation**"
      ],
      "metadata": {
        "id": "JtStgq13VFPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q langchain\n",
        "! pip install -q pypdf\n",
        "! pip install -q InstructorEmbedding sentence_transformers\n",
        "! pip install -q chromadb\n",
        "! pip install -q --upgrade together"
      ],
      "metadata": {
        "id": "7N2KbQtoVEtw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Import libraries**"
      ],
      "metadata": {
        "id": "7m8MwXBrVPf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "WMped0YkVY90"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Create an empty directory and upload the required file(s)**"
      ],
      "metadata": {
        "id": "5MJB1JHXUBxX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "5O9d0uCA85iO",
        "outputId": "a51773d7-2742-4b83-d05a-e986181886ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An empty directory named 'raw_data' has been created.\n",
            "Now please upload your required file(s).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9c9338c7-e0e3-4ad6-a124-920e59929d78\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9c9338c7-e0e3-4ad6-a124-920e59929d78\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Create an empty directory\n",
        "folder_name = \"raw_data\"\n",
        "if not os.path.exists(folder_name):\n",
        "    os.makedirs(folder_name)\n",
        "print(f\"An empty directory named '{folder_name}' has been created.\")\n",
        "\n",
        "# Upload files to the folder\n",
        "print(f\"Now please upload your required file(s).\")\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# Move uploaded files to the created folder\n",
        "for file_name in uploaded_files.keys():\n",
        "    source_path = file_name\n",
        "    destination_path = os.path.join(folder_name, file_name)\n",
        "    os.rename(source_path, destination_path)\n",
        "    print(f\"'{file_name}' has been uploaded and moved to '{folder_name}' directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Load multiple files and process the documents**"
      ],
      "metadata": {
        "id": "t0QgwSPxUyf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DirectoryLoader('./raw_data/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "zk4iAqPzUvAC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hzhqhaoXInh",
        "outputId": "3cc99751-e835-4305-ac94-bf02adbe00ad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "219"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the documents into text\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "YOPQ6bjgXYSf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Create the embeddings and store in a vector database**"
      ],
      "metadata": {
        "id": "rDXaRFfKKboL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"hkunlp/instructor-xl\"\n",
        "model_kwargs = {'device': 'cuda'}\n",
        "encode_kwargs = {'normalize_embeddings': True}\n",
        "\n",
        "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVbVYmXIKmQa",
        "outputId": "544770e2-874d-4410-f363-ab859318b9d0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import trange\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = 'vectordb'\n",
        "embedding = instructor_embeddings\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)"
      ],
      "metadata": {
        "id": "FYVzbinkQDeU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Make the retriever**"
      ],
      "metadata": {
        "id": "qKFFpUlHZQm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})"
      ],
      "metadata": {
        "id": "0m4ge3FgZbUH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**RetrievalQA with LLaMA 2-70B on Together API**"
      ],
      "metadata": {
        "id": "eLiueysNaUEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TOGETHER_API_KEY\"] = \"\" # create your own API key https://api.together.xyz/signin?callbackUrl=https%3A%2F%2Fapi.together.xyz%2Fplayground"
      ],
      "metadata": {
        "id": "9Z3bKf39aaRG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import together\n",
        "\n",
        "# set your API key\n",
        "together.api_key = os.environ[\"TOGETHER_API_KEY\"]\n",
        "\n",
        "# # list available models and descriptons\n",
        "# models = together.Models.list()\n",
        "\n",
        "# together.Models.start(\"togethercomputer/llama-2-70b-chat\")"
      ],
      "metadata": {
        "id": "x0z13ERnargM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from typing import Any, Dict, List, Mapping, Optional\n",
        "\n",
        "from pydantic import Extra, Field, root_validator\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.llms.utils import enforce_stop_tokens\n",
        "from langchain.utils import get_from_dict_or_env\n",
        "\n",
        "class TogetherLLM(LLM):\n",
        "    \"\"\"Together large language models.\"\"\"\n",
        "\n",
        "    model: str = \"togethercomputer/llama-2-70b-chat\"\n",
        "    \"\"\"model endpoint to use\"\"\"\n",
        "\n",
        "    together_api_key: str = os.environ[\"TOGETHER_API_KEY\"]\n",
        "    \"\"\"Together API key\"\"\"\n",
        "\n",
        "    temperature: float = 0.1\n",
        "    \"\"\"What sampling temperature to use.\"\"\"\n",
        "\n",
        "    max_tokens: int = 1024\n",
        "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
        "\n",
        "    class Config:\n",
        "        extra = Extra.forbid\n",
        "\n",
        "    @root_validator()\n",
        "    def validate_environment(cls, values: Dict) -> Dict:\n",
        "        \"\"\"Validate that the API key is set.\"\"\"\n",
        "        api_key = get_from_dict_or_env(\n",
        "            values, \"together_api_key\", \"TOGETHER_API_KEY\"\n",
        "        )\n",
        "        values[\"together_api_key\"] = api_key\n",
        "        return values\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of LLM.\"\"\"\n",
        "        return \"together\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"Call to Together endpoint.\"\"\"\n",
        "        together.api_key = self.together_api_key\n",
        "        output = together.Complete.create(prompt,\n",
        "                                          model=self.model,\n",
        "                                          max_tokens=self.max_tokens,\n",
        "                                          temperature=self.temperature,\n",
        "                                          )\n",
        "        text = output['output']['choices'][0]['text']\n",
        "        return text"
      ],
      "metadata": {
        "id": "xhF5VwaZbxQI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Make the chain**"
      ],
      "metadata": {
        "id": "P90aO-FRa51_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = TogetherLLM()\n",
        "\n",
        "# create the chain to answer questions\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ],
      "metadata": {
        "id": "CDBISoF6a79g"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Cite sources\n",
        "\n",
        "import textwrap\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])"
      ],
      "metadata": {
        "id": "IxFhMnSkcHn2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# full example\n",
        "query = \"can you explain what is Flash attention?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67FcZQa6cLRk",
        "outputId": "0f08bb03-9dfb-4c1e-b3ca-113aff8ec0ad"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, FlashAttention is a new attention algorithm proposed in the paper that computes exact attention with far\n",
            "fewer memory accesses. It restructures the attention computation to split the input into blocks and make\n",
            "several passes over input blocks, thus incrementally performing the softmax reduction. It also stores the\n",
            "softmax normalization factor from the forward pass to quickly recompute attention on-chip in the backward\n",
            "pass, which is faster than the standard approach of reading the intermediate attention matrix from HBM. The\n",
            "algorithm is designed to be IO-aware, accounting for reads and writes between levels of GPU memory, and it is\n",
            "shown to be faster and more memory-efficient than existing attention methods.\n",
            "\n",
            "\n",
            "Sources:\n",
            "raw_data/Flash-attention.pdf\n",
            "raw_data/Flash-attention.pdf\n",
            "raw_data/Flash-attention.pdf\n",
            "raw_data/Flash-attention.pdf\n",
            "raw_data/Flash-attention.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# full example\n",
        "query = \"can you list down the each concept of this paper?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjV5Ngx4ceqY",
        "outputId": "2365dc5c-9ea2-472d-bbfc-c54684cdf261"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here are the concepts discussed in this paper:\n",
            "1. Chain-of-thought (CoT) prompting technique for LMs.\n",
            "2. Few-shot prompting method.\n",
            "3. Multi-turn prompts.\n",
            "4. ReAct - a method that integrates decision-making and reasoning capabilities into a large language model.\n",
            "5. Intuitive and easy-to-design prompts.\n",
            "6. General and flexible prompts.\n",
            "7. Performant and robust prompts.\n",
            "8. Scratchpads - a method that allows LMs to use a working memory when more than one step is required to solve\n",
            "a task correctly.\n",
            "9. Fine-tuning on example tasks with associated computation steps.\n",
            "10. Few-shot prompting on ly requires a handful of manually labeled examples and enables very fast\n",
            "experimentation as no model Ô¨Åne-tuning is required.\n",
            "11. Ability to perform reasoning with chain-of-thoughts from a few in-context examples only emerges as models\n",
            "reach a certain size.\n",
            "12. Performance depends heavily on the format in which examples are presented, the choice of few-shot\n",
            "examples, and the order in which they are presented.\n",
            "\n",
            "\n",
            "Sources:\n",
            "raw_data/Augmenting LLMs Survey.pdf\n",
            "raw_data/llama-2-paper.pdf\n",
            "raw_data/ReACT.pdf\n",
            "raw_data/Augmenting LLMs Survey.pdf\n",
            "raw_data/Augmenting LLMs Survey.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# full example\n",
        "query = \"can you explain the concept of Chain-of-thought (CoT) prompting technique for LMs?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKOpUFhIdoG3",
        "outputId": "f16ceb85-9a6d-4be1-e2df-634a3b05be88"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sure! Chain-of-thought (CoT) is a prompting technique for large language models (LMs) that involves providing\n",
            "the model with a series of intermediate reasoning steps leading to the final output. The idea is to elicit the\n",
            "model's own \"thinking procedure\" for problem-solving, rather than simply providing the final answer.\n",
            "\n",
            "The CoT prompt typically consists of a task or question, followed by a series of prompts that ask the model to\n",
            "explain its reasoning step by step. For example, in the case of a math problem, the prompt might ask the model\n",
            "to explain how it arrived at a particular answer, or what steps it took to solve the problem.\n",
            "\n",
            "The CoT technique has been shown to be effective in a variety of domains, including arithmetic, commonsense,\n",
            "and symbolic reasoning tasks. It has also been extended to zero-shot prompting, where the model is given a\n",
            "single prompt that is not an example, and is able to generate a chain of thought to solve the task.\n",
            "\n",
            "One advantage of the CoT technique is that it allows the model to generate its own \"thinking procedure\" for\n",
            "problem-solving, rather than relying on pre-defined rules or algorithms. This can make the model more flexible\n",
            "and adaptable to new situations, and can also provide insights into how the model is thinking and making\n",
            "decisions.\n",
            "\n",
            "However, the CoT technique is not without its limitations. One challenge is that the model's chain of thought\n",
            "may not always be accurate or complete, and may require additional guidance or feedback from the user.\n",
            "Additionally, the CoT technique may not be suitable for all types of tasks or domains, and may require\n",
            "additional prompting or training to be effective.\n",
            "\n",
            "\n",
            "Sources:\n",
            "raw_data/Augmenting LLMs Survey.pdf\n",
            "raw_data/ReACT.pdf\n",
            "raw_data/ReACT.pdf\n",
            "raw_data/Augmenting LLMs Survey.pdf\n",
            "raw_data/Augmenting LLMs Survey.pdf\n"
          ]
        }
      ]
    }
  ]
}